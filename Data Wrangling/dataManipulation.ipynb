{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library used to manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = \"DatasetMasakanIndo.zip\"\n",
    "OutputAfter = \"DatasetMasakanIndo\"\n",
    "\n",
    "os.makedirs(OutputAfter, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(OutputAfter)\n",
    "    print(f\"The file was successfully extracted to the folder: {OutputAfter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View some data contents and how much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DatasetMasakanIndo/dataset-ayam.csv\")\n",
    "\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we couldn't find the data we were looking for, we decided to manually label it ourselves. \n",
    "\n",
    "Therefore, we minimize the data.\n",
    "\n",
    "The next process is to delete the Missing Value data, Duplicate data, truncate the data as mentioned earlier, and save the new data to a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirectOriData = 'DatasetMasakanIndo'\n",
    "OutputAfter = 'NewData'\n",
    "\n",
    "\n",
    "if not os.path.exists(OutputAfter):\n",
    "    os.makedirs(OutputAfter)\n",
    "\n",
    "for file in os.listdir(DirectOriData):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(DirectOriData, file)\n",
    "        print(f\"{file} file, in process\")\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_cleaned = df.dropna().drop_duplicates()\n",
    "        df_cleaned = df_cleaned.head(1000)\n",
    "        \n",
    "        output_path = os.path.join(OutputAfter, file)\n",
    "        df_cleaned.to_csv(output_path, index=False)\n",
    "        print(f\"{file} file, successfully saved in {OutputAfter}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, each file creates a Category column, where the contents of the category are based on the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = 'NewData'\n",
    "OutputAfter = 'NewData_Category'\n",
    "\n",
    "if not os.path.exists(OutputAfter):\n",
    "    os.makedirs(OutputAfter)\n",
    "\n",
    "for file in os.listdir(dataNew):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(dataNew, file)\n",
    "        print(f\"{file} file, in process\")\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'ayam' in file:\n",
    "            kategori = 'ayam'\n",
    "        elif 'ikan' in file:\n",
    "            kategori = 'ikan'\n",
    "        elif 'kambing' in file:\n",
    "            kategori = 'kambing'\n",
    "        elif 'sapi' in file:\n",
    "            kategori = 'sapi'\n",
    "        elif 'tahu' in file:\n",
    "            kategori = 'tahu'\n",
    "        elif 'telur' in file:\n",
    "            kategori = 'telur'\n",
    "        elif 'tempe' in file:\n",
    "            kategori = 'tempe'\n",
    "        elif 'udang' in file:\n",
    "            kategori = 'udang'\n",
    "        \n",
    "        df.insert(0, 'Category', kategori)\n",
    "        \n",
    "        output_path = os.path.join(OutputAfter, file)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"{file} file, successfully saved in {OutputAfter}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Followed by merging all the files, which will be given an Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = 'NewData_Category'\n",
    "OutputAfter = 'NewData_Cate_IdNull.csv'\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in os.listdir(dataNew):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(dataNew, file)\n",
    "        print(f\"{file} file merging process\")\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        all_data.append(df)\n",
    "\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "merged_data.to_csv(OutputAfter, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the process performs\n",
    "1. Id creation and assignment\n",
    "2. Putting the Id column at the front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = 'NewData_Cate_IdNull.csv'\n",
    "\n",
    "df = pd.read_csv(dataNew)\n",
    "\n",
    "df['Id'] = range(1, len(df) + 1)\n",
    "\n",
    "cols = ['Id'] + [col for col in df.columns if col != 'Id']\n",
    "df = df[cols]\n",
    "\n",
    "df.to_csv('NewData_Cate_Id.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the process performs\n",
    "1. Deletion of unused data columns, i.e. Loves column\n",
    "2. Cleaning the data in the Title, Ingredients, and Steps columns from emots\n",
    "3. Title cleaning, so that only words, numbers, and spaces are used.\n",
    "4. Making every word capitalized in the Title column\n",
    "5. Discarding data that has “\\n”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = \"NewData_Cate_Id.csv\"\n",
    "df = pd.read_csv(dataNew)\n",
    "\n",
    "df = df.drop(columns=['Loves'])\n",
    "\n",
    "def remove_emot(text):\n",
    "    regex_pattern = r\"[^\\w\\s.,'\\\"!?;:@#$%^&*()\\-+=<>/\\\\|`~\\[\\]{}]\"\n",
    "    return re.sub(regex_pattern, '', text)\n",
    "\n",
    "def clean_title(title):\n",
    "    regex_pattern = r\"[^\\w\\s]\"\n",
    "    title = re.sub(regex_pattern, '', title)\n",
    "    return title\n",
    "\n",
    "def capitalize_title(title):\n",
    "    return ' '.join(word.capitalize() for word in title.split())\n",
    "\n",
    "columns = ['Title', 'Ingredients', 'Steps']\n",
    "for i in columns:\n",
    "    df[i] = df[i].astype(str).apply(remove_emot)\n",
    "\n",
    "df['Title'] = df['Title'].astype(str).apply(clean_title)\n",
    "df['Title'] = df['Title'].astype(str).apply(capitalize_title)\n",
    "\n",
    "columns = ['Ingredients', 'Steps']\n",
    "for i in columns:\n",
    "    df = df[~df[i].str.contains(r'\\n', na=False)]\n",
    "\n",
    "df.to_csv(\"NewData_Cate_Id_Clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add Type and Temp(cold) columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = 'NewData_Cate_Id_Clean.csv'\n",
    "OutputAfter = 'NewData_Uncomplete.csv'\n",
    "\n",
    "df = pd.read_csv(dataNew)\n",
    "\n",
    "new_colomn = ['Type', 'Temp(cold)']\n",
    "for colomn in new_colomn:\n",
    "    df[colomn] = None\n",
    "\n",
    "\n",
    "df.to_csv(OutputAfter, index=False)\n",
    "\n",
    "print(f\"CSV file with new columns successfully saved to {OutputAfter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, break down the data again according to Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = 'NewData_Uncomplete.csv'\n",
    "df = pd.read_csv(dataNew)\n",
    "\n",
    "OutputAfter = 'NewData_Category_Id_Clean'\n",
    "\n",
    "if not os.path.exists(OutputAfter):\n",
    "    os.makedirs(OutputAfter)\n",
    "\n",
    "for category, group in df.groupby('Category'):\n",
    "    output_file = os.path.join(OutputAfter, f\"{category}_data.csv\")\n",
    "\n",
    "    group.to_csv(output_file, index=False)\n",
    "    print(f\"The data for category '{category}' has been saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, there is a URL column that contains the recipe link, but only the back link, not the base url.\n",
    "\n",
    "Therefore, we try to search for the base url.\n",
    "\n",
    "Once obtained, the next process is to convert the recipe link into a recipe image link. In this case, we enter the link, and then search for the tag of the image element that we want to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change each data, for example ayam => ikan, and run this code again\n",
    "#If you want a fast process, we recommend creating a new ipynb file, and copy this code with different data\n",
    "\n",
    "data = \"ayam\"\n",
    "dataNew = f'NewData_Category_Id_Clean/{data}_data.csv'\n",
    "OutputAfter = 'NewData_Category_Id_Url_Clean'\n",
    "\n",
    "if not os.path.exists(OutputAfter):\n",
    "    os.makedirs(OutputAfter)\n",
    "\n",
    "OutputAfter_name = os.path.join(OutputAfter, f'{data}_data_url.csv')\n",
    "\n",
    "base_url = \"https://cookpad.com\"\n",
    "\n",
    "def fetch_image_url(relative_url):\n",
    "    full_url = base_url + relative_url\n",
    "    try:\n",
    "        response = requests.get(full_url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            image_div = soup.find('div', class_='tofu_image')\n",
    "            if image_div:\n",
    "                img_tag = image_div.find('img')\n",
    "                if img_tag and 'src' in img_tag.attrs:\n",
    "                    return img_tag['src']\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL {full_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "df = pd.read_csv(dataNew)\n",
    "\n",
    "if 'URL' in df.columns:\n",
    "    updated_urls = []\n",
    "    for idx, relative_url in enumerate(df['URL']):\n",
    "        print(f\"Processing row {idx + 1}/{len(df)}: {relative_url}\")\n",
    "        img_url = fetch_image_url(relative_url)\n",
    "        updated_urls.append(img_url)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    df['URL'] = updated_urls\n",
    "    \n",
    "    df.to_csv(OutputAfter_name, index=False)\n",
    "    print(f\"File successfully updated and saved to: {OutputAfter}\")\n",
    "else:\n",
    "    print(\"The 'URL' field was not found in the file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the process performs\n",
    "1. Scraping the image from the link that has been obtained\n",
    "2. Changing the URL field with the same value as the image file name\n",
    "3. The whole process is made into a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is quite time-consuming, we recommend doing it as in the previous code\n",
    "\n",
    "data = \"ayam\"\n",
    "dataNew = f\"NewData_Category_Id_Url_Clean/{data}_data_url.csv\"\n",
    "OutputAfter = f\"DataPhoto/Photo_{data}\"\n",
    "OutputAfter2 = \"Data_Complete\"\n",
    "\n",
    "df = pd.read_csv(dataNew)\n",
    "\n",
    "if not os.path.exists(OutputAfter):\n",
    "    os.makedirs(OutputAfter)\n",
    "\n",
    "if not os.path.exists(OutputAfter2):\n",
    "    os.makedirs(OutputAfter2)\n",
    "\n",
    "def format_title(title):\n",
    "    return title.lower().replace(\" \", \"_\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        id_ = str(row[\"Id\"])\n",
    "        title_ = format_title(row[\"Title\"])\n",
    "        url = row[\"URL\"]\n",
    "        \n",
    "        Fname = f\"{id_}_{title_}.jpg\"\n",
    "        file_path = os.path.join(OutputAfter, Fname)\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Image downloaded successfully: {Fname}\")\n",
    "            \n",
    "            df.at[index, \"URL\"] = Fname\n",
    "        else:\n",
    "            print(f\"Failed to download image from URL: {url} (Status code: {response.status_code})\")\n",
    "            \n",
    "            df.at[index, \"URL\"] = np.nan\n",
    "        \n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error on Id {id_}: {e}\")\n",
    "        df.at[index, \"URL\"] = np.nan\n",
    "\n",
    "output_csv = os.path.join(OutputAfter2, f\"{data}_data_complete.csv\")\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded photo files are zipped so that they can be uploaded to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, manually assign values to the Type and Temp(cold) columns, which are coded in EditData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After editing each category with maximum effort, proceed with the process of\n",
    "1. Merging all categories\n",
    "2. Deleting Missing Value data and Duplicate data again\n",
    "3. Saving it into Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = 'Data_Complete/data_complete'\n",
    "OutputAfter = 'Final_Dataset.csv'\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in os.listdir(dataNew):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(dataNew, file)\n",
    "        print(f\"{file} file merging process\")\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        all_data.append(df)\n",
    "\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "merged_data = merged_data.dropna()\n",
    "\n",
    "merged_data = merged_data.drop_duplicates()\n",
    "\n",
    "merged_data.to_csv(OutputAfter, index=False)\n",
    "print(f\"New data has been saved to: {OutputAfter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was changed slightly because the symbol ( ; ) at Ingredients and Steps columns is quite problematic for the Backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = \"Final_Dataset.csv\"\n",
    "OutputAfter = \"Final_Dataset1.csv\"\n",
    "\n",
    "df = pd.read_csv(dataNew)\n",
    "\n",
    "df = df.map(lambda x: str(x).replace(';', ':') if isinstance(x, str) else x)\n",
    "\n",
    "df.to_csv(OutputAfter, index=False)\n",
    "print(f\"All ';' in the dataset has been replaced with ':'. Results are stored in: {OutputAfter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the dataset, which is still in CSV form, into JSON form, to make it easier to upload data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = 'Final_Dataset1.csv'\n",
    "\n",
    "OutputAfter = \"Final_Dataset1.json\"\n",
    "\n",
    "with open(dataNew, mode='r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile, delimiter=',')\n",
    "\n",
    "    json_data = []\n",
    "    \n",
    "    for row in reader:\n",
    "        print(f\"Header: {reader.fieldnames}\")\n",
    "        print(f\"Row Data: {row}\")\n",
    "\n",
    "        record = {\n",
    "            \"Id\": row.get(\"Id\", \"\").strip(),\n",
    "            \"Category\": row.get(\"Category\", \"\").strip(),\n",
    "            \"Title\": row.get(\"Title\", \"\").strip(),\n",
    "            \"Ingredients\": row.get(\"Ingredients\", \"\").strip(),\n",
    "            \"Steps\": row.get(\"Steps\", \"\").strip(),\n",
    "            \"URL\": row.get(\"URL\", \"\").strip(),\n",
    "            \"Type\": row.get(\"Type\", \"\").strip(),\n",
    "            \"Temp(cold)\": row.get(\"Temp(cold)\", \"\").strip()\n",
    "        }\n",
    "        \n",
    "        print(f\"Record processed: {record}\")\n",
    "        \n",
    "        if any(record.values()):\n",
    "            json_data.append(record)\n",
    "        else:\n",
    "            print(\"This line is empty and not entered:\", record)\n",
    "\n",
    "    with open(OutputAfter, mode='w', encoding='utf-8') as outfile:\n",
    "        json.dump(json_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"The file has been successfully processed and saved as '{OutputAfter}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
